{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image classification with Vision Transformer ver1.0",
      "provenance": [],
      "collapsed_sections": [
        "hUJNlS9k_t96",
        "VnLDlxf4_t98",
        "K1ClOHvR_t99",
        "Sj7SinP5_t9-",
        "3mJ18zZB_t9_",
        "l1Td-wPN_t-A",
        "iaFul3ZU_t-B"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUUTHIENXUAN/Vision-Transformer-ViT-in-Tensorflow/blob/main/Image_classification_with_Vision_Transformer_ver1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB06tF4R_t9p"
      },
      "source": [
        "# Image classification with Vision Transformer\n",
        "\n",
        "**Author:** [LUU THIEN XUAN](https://www.linkedin.com/in/thienxuanluu/)<br>\n",
        "\n",
        "**Credit:** [Phil Wang](https://github.com/lucidrains)<br>\n",
        "**Credit:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2021/01/18<br>\n",
        "**Last modified:** 2021/01/18<br>\n",
        "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWdrPHd_t94"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
        "model by Alexey Dosovitskiy et al. for image classification,\n",
        "and demonstrates it on the CIFAR-100 dataset.\n",
        "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
        "image patches, without using convolution layers.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
        "which can be installed using the following command:\n",
        "\n",
        "```python\n",
        "pip install -U tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJNlS9k_t96"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1ahDCghHpBY",
        "outputId": "80fd1ac0-5555-40b9-a9f5-0ce0f93bf3ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTIFgFRN_t97"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnLDlxf4_t98"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7-zEOsp_t99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7932ed5a-c7e8-4d0e-dc97-e64ea7957041"
      },
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1ClOHvR_t99"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceZc0uL4_t9-"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj7SinP5_t9-"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwQfcEUg_t9_"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Normalization(),\n",
        "        layers.experimental.preprocessing.Resizing(image_size, image_size),\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "        layers.experimental.preprocessing.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .cache()\n",
        "    .shuffle(1000)\n",
        "    .batch(batch_size,drop_remainder=True)\n",
        "    .repeat()\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "val_batches = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    .cache()\n",
        "    .batch(batch_size,drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "pmPssOfvzyqX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZdU-cEF_t-C"
      },
      "source": [
        "## Build the ViT model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "kbBcrJCukQCM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(layers.Layer):\n",
        "  def __init__(self, fn):\n",
        "    super(PreNorm, self).__init__()\n",
        "    self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.fn = fn\n",
        "\n",
        "  #@tf.function(jit_compile=True)  \n",
        "  def call(self, x, **kwargs):\n",
        "    return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "6vVodIZdm3Wx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(layers.Layer):\n",
        "  \n",
        "  def __init__(self, dim, hidden_dim, dropout=0.1):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net =  keras.Sequential([\n",
        "                    layers.Dense(hidden_dim, activation=tf.nn.gelu),\n",
        "                    #tfa.layers.GELU(),\n",
        "                    layers.Dropout(dropout),\n",
        "                    layers.Dense(dim),\n",
        "                    layers.Dropout(dropout)\n",
        "                    ])\n",
        "  #@tf.function(jit_compile=True)\n",
        "  def call(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "cbOWfmAHnnHO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(layers.Layer):\n",
        "\n",
        "  def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "     \n",
        "    super(Attention, self).__init__()\n",
        "    self.heads = heads\n",
        "    self.dim_head = dim_head\n",
        "\n",
        "    self.inner_dim = self.dim_head *  self.heads\n",
        "\n",
        "    self.to_q = layers.Dense(self.inner_dim)\n",
        "    self.to_k = layers.Dense(self.inner_dim)\n",
        "    self.to_v = layers.Dense(self.inner_dim)\n",
        "\n",
        "    self.scale = 1/K.sqrt(K.cast(dim_head, 'float32'))\n",
        "    self.attend = layers.Activation('softmax')\n",
        "    self.to_out = keras.Sequential([\n",
        "            layers.Dense(dim),\n",
        "            layers.Dropout(dropout)])\n",
        "   \n",
        "  #@tf.function(jit_compile=True)\n",
        "  def call(self, inputs):\n",
        "    batch_size = K.int_shape(inputs)[0] #tf.shape(inputs)[0]\n",
        "\n",
        "    q = self.to_q(inputs)\n",
        "    k = self.to_k(inputs)\n",
        "    v = self.to_v(inputs)\n",
        "\n",
        "    q = K.reshape(q, (batch_size, -1, self.heads, self.dim_head))\n",
        "    k = K.reshape(k, (batch_size, -1, self.heads, self.dim_head))\n",
        "    v = K.reshape(v, (batch_size, -1, self.heads, self.dim_head))\n",
        "\n",
        "    q = K.permute_dimensions(q, (0, 2, 1, 3))\n",
        "    k = K.permute_dimensions(k, (0, 2, 1, 3))\n",
        "    v = K.permute_dimensions(v, (0, 2, 1, 3))\n",
        "\n",
        "    dots = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "    attn = self.attend(dots)\n",
        "\n",
        "    out = tf.matmul(attn, v)\n",
        "    out = K.permute_dimensions(out, (0, 2, 1, 3))\n",
        "    out = K.reshape(out, (batch_size, -1, self.inner_dim))\n",
        "    \n",
        "    return self.to_out(out)"
      ],
      "metadata": {
        "id": "lmnDQT2oxC45"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(layers.Layer):\n",
        "  \n",
        "  def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.layers = []\n",
        "    for _ in range(depth):\n",
        "      self.layers.append(\n",
        "          [PreNorm(Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "           PreNorm(FeedForward(dim, mlp_dim, dropout = dropout))])\n",
        "  \n",
        "  #@tf.function(jit_compile=True)        \n",
        "  def call(self, x):\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) + x\n",
        "      x = ff(x) + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "eD838Yvt3XAc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "  return t if isinstance(t, tuple) else (t, t) \n",
        "\n",
        "class ViT(layers.Layer):\n",
        "  \n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, \n",
        "               pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "    \n",
        "    super(ViT, self).__init__()\n",
        "    image_height, image_width = pair(image_size)\n",
        "    patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "    assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "    self.num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "    self.patch_dim   = channels * patch_height * patch_width\n",
        "    assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "    \n",
        "    self.patch_size = patch_size\n",
        "    self.dim = dim\n",
        "    self.dense = layers.Dense(self.dim)\n",
        "\n",
        "    self.pos_embedding = self.add_weight(shape=[1, self.num_patches+1, self.dim],dtype=tf.float32) \n",
        "    self.cls_token = self.add_weight(shape=[1, 1, self.dim],dtype=tf.float32) \n",
        "    self.dropout = layers.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "    self.pool = pool\n",
        "    self.to_latent1 = layers.Dropout(0.1)\n",
        "    self.to_latent2 = layers.Dense(mlp_dim, activation=tfa.activations.gelu)\n",
        "\n",
        "    self.mlp_head = keras.Sequential([\n",
        "            layers.LayerNormalization(epsilon=1e-6),\n",
        "            layers.Dense(num_classes)]) #, activation='softmax'\n",
        "      \n",
        "  def build(self, input_shape):\n",
        "    self.b = input_shape[0]\n",
        "    super(ViT, self).build(input_shape)\n",
        "    \n",
        "  #@tf.function(jit_compile=True)\n",
        "  def call(self, inputs):\n",
        "    \n",
        "    x = tf.nn.space_to_depth(inputs, self.patch_size)\n",
        "    x = K.reshape(x, (-1, self.num_patches, self.patch_dim))\n",
        "    x = self.dense(x)\n",
        "    b = tf.shape(x)[0] #b , _ , _ = x.shape\n",
        "    \n",
        "    \"\"\"\n",
        "    cls_tokens = K.repeat_elements(self.cls_token, self.b, axis=0)\n",
        "    x = K.concatenate((cls_tokens, x), axis=1)\n",
        "    \n",
        "    pos_emb = K.repeat_elements(self.pos_embedding, self.b, axis=0)\n",
        "    \"\"\"\n",
        "    \n",
        "    cls_tokens = tf.repeat(self.cls_token, b, axis=0)\n",
        "    x = tf.concat((cls_tokens, x), axis=1)\n",
        "    \n",
        "    pos_emb = tf.repeat(self.pos_embedding, b, axis=0)  \n",
        "    \n",
        "    x += pos_emb\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "    x = self.to_latent1(self.to_latent2(x))\n",
        "    return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "qFwifd40oPPV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug model"
      ],
      "metadata": {
        "id": "mvPyb5IOdEsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 8,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "model.build(input_shape=(1,256,256,3))\n",
        "\n",
        "\n",
        "img = tf.random.uniform(shape=[1, 256, 256, 3])\n",
        "preds = model(img)\n",
        "print(preds.shape) # (1, 1000)"
      ],
      "metadata": {
        "id": "6Dz8qBXE45XV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f2f508-b1f2-4eea-9c72-24b249191491"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAf2Eujt_t-D"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnLazE_I_t-E"
      },
      "source": [
        "def run_experiment(model):\n",
        "  \n",
        "  optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "  model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "  checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "  checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "  history = model.fit(\n",
        "        train_batches,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch = len(x_train)//batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=val_batches,\n",
        "        validation_steps= len(x_test)//batch_size, \n",
        "        #validation_split=0.1,\n",
        "        #callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "  return history"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class vit_classifier(keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    \n",
        "    super(vit_classifier, self).__init__()\n",
        "    self.aug = data_augmentation\n",
        "    self.vit = ViT(image_size = image_size,\n",
        "                   patch_size = patch_size,\n",
        "                   num_classes = num_classes,\n",
        "                   dim = 1024,\n",
        "                   depth = transformer_layers,\n",
        "                   heads = num_heads,\n",
        "                   dim_head = 16,\n",
        "                   mlp_dim = 2048,\n",
        "                   dropout = 0.1,\n",
        "                   emb_dropout = 0.1\n",
        "                )\n",
        "    \n",
        "      \n",
        "  def call(self, inputs):\n",
        "    x = self.aug(inputs)\n",
        "    return self.vit(x)"
      ],
      "metadata": {
        "id": "EIIw8x8mk_Jg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = vit_classifier()\n",
        "classifier.build(input_shape=(batch_size,32,32,3))\n",
        "history = run_experiment(classifier)"
      ],
      "metadata": {
        "id": "ysiVLCp9mBJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiVmT4gp_t-F"
      },
      "source": [
        "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
        "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
        "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
        "\n",
        "Note that the state of the art results reported in the\n",
        "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
        "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
        "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
        "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
        "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
        "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
        "In practice, it's recommended to fine-tune a ViT model\n",
        "that was pre-trained using a large, high-resolution dataset."
      ]
    }
  ]
}